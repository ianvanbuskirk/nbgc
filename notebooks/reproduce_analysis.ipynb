{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import unidecode\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure data is downloaded and available\n",
    "for folder in ['name_data', 'replication_data', 'figures']:\n",
    "    if folder not in os.listdir('../'):\n",
    "        os.mkdir('../%s'%folder)\n",
    "        \n",
    "def download_if_necessary(folder, file):\n",
    "    if file not in os.listdir('../%s'%folder):\n",
    "        subprocess.run(['osf', '-p', 'tz38q', 'fetch', \n",
    "                        '%s/%s'%(folder, file), '%s/%s'%(folder, file)], cwd=\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='Helvetica')\n",
    "\n",
    "combined_colors = {'purple':(40, 37, 70),'lightpurple':(84, 88, 116),\n",
    "                   'blue':(84, 130, 154),'lightblue':(165, 188, 198),\n",
    "                   'green':(66, 81, 45),'lightgreen':(164, 196, 99, ), # green: (120, 151, 59, )\n",
    "                   'yellow':(244, 199, 72, ),'lightyellow':(245, 230, 98),\n",
    "                   'orange':(230, 121, 62),'lightorange':(255,157,72),\n",
    "                   'red':(135, 41, 43),'lightred':(185, 63, 63)\n",
    "                   }\n",
    "combined_colors = {c:np.array(v)/256 for c,v in combined_colors.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_if_necessary('name_data', 'source-aggregated_name-gender_associations.json')\n",
    "source_associations = json.load(open('../name_data/source-aggregated_name-gender_associations.json','r'))\n",
    "\n",
    "download_if_necessary('name_data', 'averaged_name-gender_estimates.json')\n",
    "avg_estimates = json.load(open('../name_data/averaged_name-gender_estimates.json','r'))\n",
    "\n",
    "# convert dictionary to dataframe\n",
    "avg_estimates_df = pd.DataFrame(avg_estimates).T\n",
    "\n",
    "download_if_necessary('replication_data', 'country-decade-aggregated_estimates.json')\n",
    "aggregates = json.load(open('../name_data/country-decade-aggregated_estimates.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactively find names at different levels of coverage\n",
    "- finneus: $10^2$\n",
    "- garreth $10^3$\n",
    "- Abdellah $10^4$\n",
    "- madelyn $10^5$\n",
    "- alexander $10^6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "l,h = 10**i + np.array([-(10**(i-1)), 10**(i-1)])\n",
    "level_df = avg_estimates_df.loc[(avg_estimates_df['M_counts'] > l)&\n",
    "                                (avg_estimates_df['M_counts'] < h)].copy()\n",
    "level_df['d'] = abs(level_df['M_counts'] - 10**i)\n",
    "# level_df.sort_values(by='d').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a name more strongly gendered female in USA and more strongly gendered male globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_switches = []\n",
    "for name, counts in source_associations.items():\n",
    "    try:\n",
    "        us_estimate = counts['1']['f']/(counts['1']['f'] + counts['1']['m'])\n",
    "        if us_estimate > .7:\n",
    "            avg_estimate = avg_estimates[name]['AVG_estimate']\n",
    "            if avg_estimate < .3:\n",
    "                candidate_switches.append((name,int(avg_estimates[name]['M_counts'])))\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print(sorted(candidate_switches, key = lambda x: x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in ['santamaría_benchmark.csv', 'vogel_benchmark.csv', 'morgan_benchmark.csv']:\n",
    "    download_if_necessary('replication_data', file)\n",
    "\n",
    "santa = pd.read_csv('../replication_data/santamaría_benchmark.csv',\n",
    "                            keep_default_na=False)\n",
    "vogel = pd.read_csv('../replication_data/vogel_benchmark.csv',\n",
    "                            keep_default_na=False)\n",
    "morgan = pd.read_csv('../replication_data/morgan_benchmark.csv', # osf version anonymized \n",
    "                               keep_default_na=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [(santa, '36'), (vogel, '32'), (morgan, '-1')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomize Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_aggregate(d):\n",
    "    return np.array([v[:2] for v in d.values()])\n",
    "\n",
    "\n",
    "def parse_name(n):\n",
    "    return unidecode.unidecode(str(n)).lower().strip().split(' ')[0]\n",
    "\n",
    "\n",
    "def validation_average(n,ns):\n",
    "    try:\n",
    "        estimates = [v['f']/(v['f']+v['m'])\n",
    "             for k, v in source_associations[n].items() if k != ns]\n",
    "\n",
    "        return np.mean(estimates) if len(estimates) > 0 else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def compute_uncertainty(estimates_pops):\n",
    "    estimates, pops = estimates_pops.T\n",
    "    weights = pops/pops.sum()\n",
    "    return np.dot(.5 - abs(.5 - estimates),weights)\n",
    "\n",
    "\n",
    "def compute_meta_data(n, ns):\n",
    "    try:\n",
    "        n_data = avg_estimates[n]\n",
    "        e = validation_average(n, ns)\n",
    "        count = n_data['M_counts']\n",
    "        e_u = compute_uncertainty(np.array([[e,1]]))\n",
    "    except:\n",
    "        e = np.nan\n",
    "        count = 0\n",
    "        e_u = .5\n",
    "    try:\n",
    "        c_u = compute_uncertainty(parse_aggregate(aggregates[n]['c']))\n",
    "    except:\n",
    "        c_u = np.nan\n",
    "    try:\n",
    "        d_u = compute_uncertainty(parse_aggregate(aggregates[n]['d']))\n",
    "    except:\n",
    "        d_u = np.nan\n",
    "    \n",
    "    return (e, count, (e_u, c_u, d_u))\n",
    "\n",
    "\n",
    "def taxonomize_name(n, ns, min_count, max_u):\n",
    "    e, c, u = compute_meta_data(parse_name(n), ns)\n",
    "    if np.isnan(e):\n",
    "        return 'No Data'\n",
    "    elif u[0] <= max_u:\n",
    "        if c >= min_count:\n",
    "            return 'Gendered (high coverage)'\n",
    "        else:\n",
    "            return 'Gendered (low coverage)'\n",
    "    else:\n",
    "        if u[1] <= max_u:\n",
    "            return 'Conditionally Gendered (country)'\n",
    "        elif u[2] <= max_u:\n",
    "            return 'Conditionally Gendered (decade)'\n",
    "        return 'Weakly Gendered'\n",
    "    \n",
    "    \n",
    "def taxonomize(names, ns = '-1', min_count = 10, max_u = .1):\n",
    "    return [taxonomize_name(n,ns,min_count,max_u) for n in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for df, s in benchmarks:\n",
    "    df['l'] = taxonomize(df['first_name'], s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = 100 * pd.concat((pd.DataFrame(santa.groupby('l').count()['first_name']/len(santa)),\n",
    "pd.DataFrame(vogel.groupby('l').count()['first_name']/len(vogel)),\n",
    "pd.DataFrame(morgan.groupby('l').count()['first_name']/len(morgan))), axis = 1)\n",
    "tx.columns = ['santa','vogel','morgan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels  = ['Santamaría', 'Vogel', 'Morgan']\n",
    "\n",
    "tx_labels = ['Gendered (high coverage)',\n",
    "             'Gendered (low coverage)',\n",
    "             'Conditionally Gendered (country)',\n",
    "             'Conditionally Gendered (decade)',\n",
    "             'Weakly Gendered',\n",
    "             'No Data']\n",
    "\n",
    "coarse_tx_labels = ['No Data', 'Weakly Gendered',\n",
    "                      'Conditionally Gendered', 'Gendered (low coverage)',\n",
    "                      'Gendered (high coverage)']\n",
    "\n",
    "coarse_tx = tx.copy()\n",
    "coarse_tx.loc['Conditionally Gendered'] = coarse_tx.loc[[l for l in tx_labels if 'Con' in l]].sum(axis=0)\n",
    "coarse_tx = coarse_tx.drop([l for l in tx_labels if 'con' in l]).loc[coarse_tx_labels]\n",
    "coarse_tx.columns = dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx.round(1).loc[tx_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # most common gender (high coverage) names\n",
    "# Counter(pd.concat([df.loc[df['l'] == 'Gendered (high coverage)'] for df,\n",
    "#                    _ in benchmarks])['first_name'].values).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_colors = tx_colors = [combined_colors[x] for x in ['red','yellow','orange','green','blue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,10))\n",
    "\n",
    "width = .7\n",
    "prev = np.array([0.,0.,0.])\n",
    "\n",
    "for c,vals in zip(comp_colors,coarse_tx.values):\n",
    "\n",
    "    ax.bar(dataset_labels, vals, width, bottom=prev, color=c,alpha=.8)\n",
    "    prev += vals\n",
    "\n",
    "ax.bar(dataset_labels, [100,100,100], width, color='none', edgecolor=combined_colors['purple'],linewidth=1,alpha=.8)\n",
    "\n",
    "\n",
    "plt.xlim(-.6,2.6)\n",
    "plt.ylim(0,103)\n",
    "\n",
    "# Hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.ylabel('Percentage of Dataset', fontsize=18)\n",
    "\n",
    "ax.text(-0.24, 1.03, 'A', transform=ax.transAxes, fontname='Helvetica',\n",
    "                    fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "plt.savefig('../figures/dataset_taxonomy_breakdown.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute CCT Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_CCT(bin_responses):\n",
    "    N = bin_responses.shape[0]\n",
    "    M = (~np.isnan(bin_responses)).sum(axis=1)\n",
    "\n",
    "    competencies = np.ones(N)*.9\n",
    "    consensus = np.zeros(M.shape[0])\n",
    "\n",
    "    def update_consensus():\n",
    "        c = np.nanprod(bin_responses*competencies.reshape(N,-1) +\n",
    "                       (1-bin_responses)*(1-competencies).reshape(N,-1),axis=0)\n",
    "        e = np.nanprod(bin_responses*(1-competencies).reshape(N,-1) +\n",
    "                       (1-bin_responses)*competencies.reshape(N,-1),axis=0)\n",
    "\n",
    "        return(c / (c+e))\n",
    "\n",
    "    def update_competencies():\n",
    "        return(np.nansum(bin_responses*consensus.reshape(1,-1) +\n",
    "                         (1-bin_responses)*(1-consensus).reshape(1,-1),axis=1)/M)\n",
    "    \n",
    "    for i in range(100):\n",
    "        previous = competencies\n",
    "        \n",
    "        consensus = update_consensus()\n",
    "        competencies = update_competencies()\n",
    "        if np.sum((previous - competencies)**2) < 1e-10:\n",
    "            break\n",
    "        \n",
    "    consensus = update_consensus()\n",
    "    return(consensus,competencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_run_CCT(names, threshold = .1, N = 36, drop = []):\n",
    "    names = [parse_name(n) for n in names]\n",
    "    sources = np.ones(N)\n",
    "    for s in drop:\n",
    "        sources[int(s)-1] = 0\n",
    "    \n",
    "    estimated_names = []\n",
    "    all_estimates = []\n",
    "    \n",
    "    j = 0\n",
    "    for i,n in enumerate(names):\n",
    "        estimates = np.nan*np.zeros(N)\n",
    "        \n",
    "        try:\n",
    "            ratios = {s:v['f']/(v['f']+v['m']) for s, v in source_associations[n].items() if sources[int(s)-1] == 1}\n",
    "            if len(ratios.items()) == 0:\n",
    "                estimated_names.append([n,i,-1])\n",
    "            else:\n",
    "                for s,v in ratios.items():                        \n",
    "\n",
    "                    if v < (.5-threshold):\n",
    "                        estimates[int(s)-1] = 0\n",
    "                    elif v > (.5+threshold):\n",
    "                        estimates[int(s)-1] = 1\n",
    "                            \n",
    "                all_estimates.append(estimates)\n",
    "                estimated_names.append([n,i,j])\n",
    "                j += 1\n",
    "        except:\n",
    "            estimated_names.append([n,i,-1])\n",
    "            \n",
    "    cct_estimates, _ = EM_CCT(np.array(all_estimates).T[sources.astype(bool)])\n",
    "    final_cct = [cct_estimates[k] if k != -1 else np.nan for i,j,k in estimated_names]\n",
    "    return final_cct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ratio_to_class(r,threshold=0.):\n",
    "    if np.isnan(r):\n",
    "        return('-')\n",
    "    elif r < (.5 - threshold):\n",
    "        return('m')\n",
    "    elif r > (.5 + threshold):\n",
    "        return('f')\n",
    "    else:\n",
    "        return('u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2.B\n",
    "Note: Anonymized data causes a minor computational issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,s in benchmarks:\n",
    "    df['avg'] = [validation_average(parse_name(n),s) for n in df['first_name']]\n",
    "    df['avg_b'] = [convert_ratio_to_class(r) for r in df['avg'].values]\n",
    "    df['cct'] = prep_run_CCT(df['first_name'], drop = [s] if s != '-1' else [])\n",
    "    df['cct_b'] = [convert_ratio_to_class(r) for r in df['cct'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cct_metrics = {k:[] for k in dataset_labels}\n",
    "for k,(df,_) in zip(dataset_labels,benchmarks):\n",
    "    for sub in list((tx_labels,tx_labels[0:1],tx_labels[1:2],tx_labels[2:4],tx_labels[4:5],tx_labels[5:])):\n",
    "        preds, targets = df.loc[df['l'].isin(sub),['cct_b','gender']].values.T\n",
    "        num = (preds == targets).sum()\n",
    "        den = sum([p in ['m','f'] for p in preds])\n",
    "        cct_metrics[k].append([num,den,len(targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,10))\n",
    "colors = [combined_colors[x] for x in ['purple','blue','green','orange','yellow','red']]\n",
    "\n",
    "text_locs = [94.5,99,90,80,70]\n",
    "labels = ['All Names','Gendered\\nhigh coverage','Gendered\\nlow coverage','Conditionally\\nGendered','Weakly\\nGendered']\n",
    "for i,t in enumerate(labels):\n",
    "    alpha = 1 if i != 0 else .5\n",
    "    correspondence = 100*np.array([v[i][0]/v[i][1] for k,v in cct_metrics.items()])\n",
    "    \n",
    "    plt.scatter([0,1,2],\n",
    "                correspondence,\n",
    "                s=400,\n",
    "                facecolor = (1,1,1,1),\n",
    "                edgecolor = (1,1,1,1),\n",
    "                zorder=3,\n",
    "                marker='o')\n",
    "    plt.scatter([0,1,2],\n",
    "                correspondence,\n",
    "                s=400,\n",
    "                facecolor = tuple(colors[i])+tuple([alpha/2]),\n",
    "                edgecolor = tuple(colors[i])+tuple([alpha]),\n",
    "                zorder=6,\n",
    "                marker='o')\n",
    "    \n",
    "    plt.annotate(t,xy=(-1.6,text_locs[i]),# + np.mean(correspondence)\n",
    "                 horizontalalignment='center',va='center',\n",
    "                 c=tuple(colors[i])+tuple([alpha]))\n",
    "\n",
    "\n",
    "plt.xlim(-2.6,2.6)\n",
    "# plt.xlim(-.6,5.6)\n",
    "plt.ylim(68,101)\n",
    "\n",
    "plt.xticks([0,1,2],['S', 'V', 'M']);\n",
    "\n",
    "# Hide the right and top spines\n",
    "# ax.spines['right'].set_visible(True)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# limit length of x spine\n",
    "ax.spines['bottom'].set_bounds(-.4,2.6)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('right')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.ylabel('Percentage of Classified Matching Targets (Correspondence)')\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "ax.text(0.4, 1.03, 'B', transform=ax.transAxes, fontname='Helvetica',\n",
    "                    fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "# grid\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(axis='x',zorder=-1,alpha=.3,ls='-')\n",
    "\n",
    "\n",
    "plt.savefig('../figures/dataset_taxonomy_performance.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Metrics for Section on Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance numbers across taxonomy\n",
    "p_df = []\n",
    "for i,t in enumerate(labels):\n",
    "    p_df.append([t]+ list(100*np.array([v[i][0]/v[i][1] for k,v in cct_metrics.items()])))\n",
    "pd.DataFrame(p_df,columns=['l','s','v','m']).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# performance change on conditionally gendered names when jean and robin are dropped\n",
    "for df,_ in benchmarks:\n",
    "    print('-')\n",
    "    conditioned = df.loc[(df['l'].str.contains('Conditionally Gendered'))&(df['cct_b'].isin(['m','f']))]\n",
    "    dropped = conditioned.loc[~conditioned['first_name'].isin(['jean','robin'])]\n",
    "    print(np.round(100*np.mean(conditioned['cct_b'] == conditioned['gender']),0))\n",
    "    print(np.round(100*np.mean(dropped['cct_b'] == dropped['gender']),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % not classified\n",
    "for df,_ in benchmarks:\n",
    "    print(100*(1 - np.mean(df['cct_b'].isin(['m','f']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of difference in performance explained by differences in composition\n",
    "var_explained = []\n",
    "for i,origin in enumerate(['Santamaría','Vogel','Morgan']):\n",
    "    for j,target in enumerate(['Santamaría','Vogel','Morgan']):\n",
    "        if i < j:\n",
    "            o_percents = coarse_tx.copy()[origin].values[::-1]\n",
    "            t_percents = coarse_tx.copy()[target].values[::-1]\n",
    "            d_percents = o_percents - t_percents\n",
    "            x = np.dot(o_percents,np.array([x[0]/x[2] for x in cct_metrics[origin][1:]]))\n",
    "            y = np.dot(t_percents,np.array([x[0]/x[2] for x in cct_metrics[target][1:]]))\n",
    "            o_percents[0] += sum(d_percents[-2:])\n",
    "            o_percents[-2:] -= d_percents[-2:]\n",
    "            z = np.dot(o_percents,np.array([x[0]/x[2] for x in cct_metrics[origin][1:]]))\n",
    "            var_explained.append(1 - abs(y - z)/abs(y - x))\n",
    "print(np.mean(var_explained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Classifier Comparison Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_if_necessary('replication_data', 'classifier_comparison_data.json')\n",
    "class_comps = json.load(open('../replication_data/classifier_comparison_data.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_labels = ['cct']+['genderize', 'api', 'onograph', 'namsor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_metrics = {}\n",
    "comp_metrics['cct'] = cct_metrics['Santamaría']\n",
    "for k in comp_labels[1:]:\n",
    "    comp_metrics[k] = []\n",
    "    santa[k+'_b'] = [convert_ratio_to_class(r) for r in class_comps[k]['santa']]\n",
    "    for sub in list((tx_labels,tx_labels[0:1],tx_labels[1:2],tx_labels[2:4],tx_labels[4:5],tx_labels[5:])):\n",
    "        preds, targets = santa.loc[santa['l'].isin(sub),[k+'_b','gender']].values.T\n",
    "        num = (preds == targets).sum()\n",
    "        den = sum([p in ['m','f'] for p in preds])\n",
    "        comp_metrics[k].append([num,den,len(targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(3,10))\n",
    "plt.rcParams['font.size'] = 14\n",
    "colors = [combined_colors[x] for x in ['purple','blue','green','orange','yellow','red']]\n",
    "\n",
    "markers = ['h','o','o','o','o']\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    correspondence = 100*np.array([v[i][0]/v[i][1] for k,v in comp_metrics.items()])\n",
    "    for x,y in enumerate(correspondence):\n",
    "        plt.scatter(x,\n",
    "                    y,\n",
    "                    s=400,\n",
    "                    facecolor = tuple(colors[i])+tuple([.5]),\n",
    "                    edgecolor = tuple(colors[i])+tuple([1]) if x != 0 else (0,0,0,1),\n",
    "                    marker=markers[x],zorder=6)\n",
    "        plt.scatter(x,\n",
    "                    y,\n",
    "                    s=400,\n",
    "                    facecolor = (1,1,1,1),\n",
    "                    edgecolor = (1,1,1,1),\n",
    "                    marker=markers[x],zorder=3)\n",
    "        \n",
    "        \n",
    "text_locs = [96,97.5,88,77,69.5]\n",
    "labels = ['All Names','Gendered high coverage','Gendered\\nlow coverage','Conditionally\\nGendered','Weakly\\nGendered']\n",
    "for i,(y,t) in enumerate(zip(text_locs,labels)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    plt.annotate(t,xy=(2,y),\n",
    "                 horizontalalignment='center',va='center',fontsize=14,\n",
    "                 c=tuple(colors[i])+tuple([1]))\n",
    "        \n",
    "\n",
    "plt.xlim(-.5,4.5)\n",
    "plt.ylim(65,100.5)\n",
    "\n",
    "ax.scatter([.5],[65],100,\n",
    "           color='white',clip_on=False,zorder=10)\n",
    "  \n",
    "plt.xticks([0,1,2,3,4],['CCT','','','',''])\n",
    "plt.xlabel('Alt. Classifiers',labelpad=10)\n",
    "ax.xaxis.set_label_coords(.5+.1,-.0125)\n",
    "\n",
    "\n",
    "# Hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "# grid\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(axis='x',zorder=-1,alpha=.3,ls='-')\n",
    "\n",
    "plt.ylabel('Correspondence')\n",
    "\n",
    "ax.text(-0.29, 1.03, 'A', transform=ax.transAxes, fontname='Helvetica',\n",
    "                    fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "plt.savefig('../figures/classifier_taxonomy_performance.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance numbers across taxonomy\n",
    "p_df = []\n",
    "for i,t in enumerate(labels+['No Data']):\n",
    "    p_df.append([t]+ list(100*np.array([v[i][0]/v[i][1] if v[i][1] != 0 else 0 for k,v in comp_metrics.items()])))\n",
    "pd.DataFrame(p_df).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we have no data, what % does at least one paid service have no data\n",
    "some_miss = ((santa.loc[santa['l'] == 'No Data',['genderize_b','api_b','onograph_b','namsor_b']] == '-').sum(axis=1) != 0).sum()\n",
    "some_miss / santa.loc[santa['l'] == 'No Data'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures 3.B & 3.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "colors = [combined_colors[x] for x in ['purple','blue','green','orange','yellow','red']]\n",
    "\n",
    "markers = ['h','o','o','o','o']\n",
    "\n",
    "gs_kw = dict(height_ratios=[1, 1],hspace=0.2, wspace=0.05)\n",
    "fig, axd = plt.subplot_mosaic([['ur'],['b']],\n",
    "                              gridspec_kw=gs_kw, figsize=(3, 10),)\n",
    "\n",
    "axd['ur'].spines['right'].set_visible(False)\n",
    "axd['ur'].spines['top'].set_visible(False)\n",
    "\n",
    "axd['b'].spines['right'].set_visible(False)\n",
    "axd['b'].spines['top'].set_visible(False)\n",
    "axd['b'].yaxis.set_ticks_position('left')\n",
    "axd['b'].xaxis.set_ticks_position('bottom')\n",
    "\n",
    "for i,t in enumerate(comp_labels):\n",
    "    mets = comp_metrics[t]\n",
    "    axd['ur'].scatter(100*mets[0][1]/mets[0][2],\n",
    "                100*mets[0][0]/mets[0][1],\n",
    "                s=400,\n",
    "                facecolor = tuple(colors[0])+tuple([.5]),\n",
    "                edgecolor = tuple(colors[0])+tuple([1]),\n",
    "                marker=markers[i])\n",
    "    \n",
    "    if i != 0:\n",
    "        axd['b'].scatter(100*mets[-1][1]/mets[-1][2],\n",
    "                    100*mets[-1][0]/mets[-1][1],\n",
    "                    s=400,\n",
    "                    facecolor = tuple(colors[-1])+tuple([.5]),\n",
    "                    edgecolor = tuple(colors[-1])+tuple([1]),\n",
    "                    marker=markers[i])\n",
    "    \n",
    "axd['ur'].annotate('All Names',xy=(97.5,94.25),\n",
    "                 horizontalalignment='center',va='center',fontsize=14,\n",
    "                 c=tuple(colors[0])+tuple([1]))\n",
    "    \n",
    "axd['b'].annotate('No Data',xy=(62,77),\n",
    "                 horizontalalignment='center',va='center',fontsize=14,\n",
    "                 c=tuple(colors[-1])+tuple([1]))\n",
    "\n",
    "axd['ur'].set_xlim(96.6,100.3)\n",
    "axd['b'].set_xlim(47,104)\n",
    "\n",
    "axd['ur'].set_ylim(93.5,95.7)\n",
    "axd['b'].set_ylim(70.7,90)\n",
    "\n",
    "plt.sca(axd['ur'])\n",
    "plt.xticks(np.arange(97,101),[97,98,99,100]); \n",
    "\n",
    "plt.yticks([94,95],[94,95]); \n",
    "plt.ylabel('Correspondence')\n",
    "\n",
    "plt.sca(axd['b'])\n",
    "plt.yticks(np.arange(72,89,4),np.arange(72,89,4)); \n",
    "plt.ylabel('Correspondence')\n",
    "\n",
    "axd['ur'].set_xlabel('Percentage Classified',labelpad=5)\n",
    "axd['b'].set_xlabel('Percentage Classified',labelpad=5)\n",
    "\n",
    "axd['ur'].annotate('CCT',(95.5,95.15))\n",
    "\n",
    "axd['ur'].text(-0.1225, 1.03, 'B', transform=axd['ur'].transAxes, fontname='Helvetica',\n",
    "                    fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "axd['b'].text(-0.1225, 1.03, 'C', transform=axd['b'].transAxes, fontname='Helvetica',\n",
    "                    fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "plt.savefig('../figures/classifier_taxonomy_classified.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % no data names classified\n",
    "[v[-1][1]/v[-1][2] for k,v in comp_metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[v[0][1]/v[0][2] for k,v in comp_metrics.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3.D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [.05,.15,.25,.35,.5]\n",
    "cut_labels = ['0.95 - 1.00', '0.85 - 0.94', '0.75 - 0.84','0.65 - 0.74','0.50 - 0.64','No Data']\n",
    "\n",
    "x = santa['avg'].values\n",
    "err = (.5 - abs(.5 - x))\n",
    "santa['err'] = err\n",
    "santa['n'] = 'No Data'\n",
    "santa.loc[err <= cutoffs[0], 'n'] = '0.95 - 1.00'\n",
    "santa.loc[(err > cutoffs[0])&(err <= cutoffs[1]), 'n'] = '0.85 - 0.94'\n",
    "santa.loc[(err > cutoffs[1])&(err <= cutoffs[2]), 'n'] = '0.75 - 0.84'\n",
    "santa.loc[(err > cutoffs[2])&(err <= cutoffs[3]), 'n'] = '0.65 - 0.74'\n",
    "santa.loc[(err > cutoffs[3])&(err <= cutoffs[4]), 'n'] = '0.50 - 0.64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_metrics = {}\n",
    "for k in comp_labels:\n",
    "    cut_metrics[k] = []\n",
    "    for sub in cut_labels:\n",
    "        preds, targets = santa.loc[(santa['n'] == sub)&(santa['first_name']!='jean'),\n",
    "                                   [k+'_b','gender']].values.T\n",
    "        num = (preds == targets).sum()\n",
    "        den = sum([p in ['m','f'] for p in preds])\n",
    "        cut_metrics[k].append([num,den,len(targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jean_performance = 100*np.mean(santa.loc[santa['first_name'] == 'jean','gender'].values == 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "colors = [(i/10,i/10,i/10,1) for i in [2,3,4,6,8]][::-1]\n",
    "\n",
    "gs_kw = dict(width_ratios=[1, 1],hspace=0.2, wspace=0.05)\n",
    "fig, axd = plt.subplot_mosaic([['l', 'r']],\n",
    "                              gridspec_kw=gs_kw, figsize=(6, 10),)\n",
    "\n",
    "ax = axd['l']\n",
    "\n",
    "markers = ['h','o','o','o','o']\n",
    "\n",
    "ax.scatter(0,\n",
    "            jean_performance,\n",
    "            color=combined_colors['orange'],marker='h')\n",
    "\n",
    "\n",
    "for i,_ in enumerate(cut_labels[:-1]):\n",
    "    for j,t in enumerate(comp_labels):\n",
    "        mets = cut_metrics[t][i]\n",
    "        c = tuple(colors[i])\n",
    "        ax.scatter(j,\n",
    "                    100*mets[0]/mets[1],\n",
    "                    s=400,\n",
    "                    facecolor = c,\n",
    "                    edgecolor = (0,0,0,1),lw=.5 if j != 0 else 1,\n",
    "                    marker=markers[j],zorder=2.5)\n",
    "        \n",
    "plt.sca(axd['l'])      \n",
    "plt.xlim(-.5,4.5)\n",
    "plt.ylim(50,101)\n",
    "plt.ylabel('Correspondence')\n",
    "\n",
    "\n",
    "plt.xticks([0,1,2,3,4],['CCT','','','',''])\n",
    "plt.xlabel('Alt. Classifiers',labelpad=10)\n",
    "ax.xaxis.set_label_coords(.5+.1,-.0125)\n",
    "\n",
    "\n",
    "plt.annotate('jean',(.5,jean_performance-1.4),\n",
    "             rotation=0,ha='center',color=combined_colors['orange'])# va='center'\n",
    "\n",
    "# Hide the right and top spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['bottom'].set_bounds(-.5,4.5)\n",
    "\n",
    "# Only show ticks on the left and bottom spines\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "\n",
    "# grid\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(axis='x',zorder=-1,alpha=.3,ls='-')\n",
    "\n",
    "ax.scatter([.5],[50],100,\n",
    "           color='white',clip_on=False,zorder=10)\n",
    "\n",
    "ax = axd['r']\n",
    "plt.sca(axd['r']) \n",
    "\n",
    "    \n",
    "plt.rcParams['font.size'] = 10\n",
    "for c,a,b,l,d,t in zip(colors[1:-1],cutoffs[:-2],\n",
    "                 cutoffs[1:-1],\n",
    "                 [125,125,125],\n",
    "                 [.2,.2,.2,],\n",
    "                 [\"$95>y\\geq85$\", \"$85>y\\geq75$\", \"$75>y\\geq65$\"]):\n",
    "    plt.annotate(\"\",xy=(125, 100*(1-a)),zorder=0,\n",
    "                xytext=(125, 100*(1-b)),color=c,\n",
    "                arrowprops=dict(arrowstyle=\"-\", color=c,lw=1,\n",
    "                                shrinkA=5, shrinkB=5,\n",
    "                                patchA=None, patchB=None,\n",
    "                                connectionstyle=\"bar,fraction=%f\"%0),\n",
    "                                )\n",
    "    plt.annotate(t,(l+100,50*(1-a) + 50*(1-b)),rotation=-90, va='center',ha='center')\n",
    "plt.rcParams['font.size'] = 14   \n",
    "\n",
    "N, bins, patches = plt.hist(100*(1-santa.loc[santa['first_name']!='jean','err']),\n",
    "                            bins=50,edgecolor='black', linewidth=.25,orientation='horizontal',zorder=3);\n",
    "\n",
    "for c,(l,h) in enumerate(zip(cutoffs,[0]+cutoffs[:-1])):\n",
    "    for i in np.arange(len(bins))[(bins >= 100*(1-l)) & (bins < 100*(1-h))]:\n",
    "        patches[i].set_facecolor(colors[c])    \n",
    "\n",
    "jean_err = 100 * (1 - santa.loc[santa['first_name'] == 'jean','err'].values[0])\n",
    "plt.annotate('jean',\n",
    "             (80,jean_err),color=combined_colors['orange'],ha='left', va='center')# va='center'\n",
    "plt.plot([0,santa.loc[santa['first_name'] == 'jean'].shape[0]],\n",
    "         [jean_err,jean_err],alpha=1,zorder=1,lw=1,color=combined_colors['orange'])\n",
    "\n",
    "\n",
    "plt.ylim(50,101)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.yticks([],[])\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.ylabel('Estimate of How Strongly a Name is Gendered', rotation=-90)\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.yaxis.set_label_coords(.9,.5)\n",
    "\n",
    "axd['l'].text(-0.135, 1.03, 'D', transform=axd['l'].transAxes, fontname='Helvetica',\n",
    "                    fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "plt.savefig('../figures/classified_correspondence_bins_counts.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4\n",
    "Note: Anonymized data does somewhat poorly here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_if_necessary('replication_data', 'genni_ethnea.json')\n",
    "genni_ethnea = json.load(open('../replication_data/genni_ethnea.json','r'))\n",
    "\n",
    "ethnea_codes = dict(zip(\n",
    "    ['VIETNAMESE','CARIBBEAN', 'INDIAN', 'INDONESIAN', 'ARAB', 'GREEK','POLYNESIAN','BALTIC', 'AFRICAN', \n",
    "     'TURKISH', 'ENGLISH', 'GERMAN','SLAV', 'THAI', 'CHINESE', 'KOREAN', 'NORDIC', 'FRENCH', 'DUTCH', \n",
    "     'HUNGARIAN', 'ITALIAN', 'JAPANESE','ISRAELI', 'ROMANIAN','HISPANIC','MONGOLIAN','TOOSHORT','UNKNOWN','ERROR'],\n",
    "    ['VNM','CUB', 'IND', 'IDN', 'EGY', 'GRC', 'PYF', 'LTU', 'NGA', \n",
    "     'TUR', 'GBR', 'DEU','RUS', 'THA', 'CHN', 'KOR', 'SWE', 'FRA', 'NLD', \n",
    "     'HUN', 'ITA', 'JPN', 'ISR', 'ROU','ESP','MNG','XXX','XXX','XXX']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_genni_estimates(df):\n",
    "    genni = []\n",
    "    for f,l in df[['first_name','last_name']].values.astype(str):\n",
    "        try:\n",
    "            response = genni_ethnea[f+'+'+l]\n",
    "            genni.append(response['Genni'].lower())\n",
    "        except:\n",
    "            genni.append('-')\n",
    "    return genni\n",
    "\n",
    "\n",
    "def compute_conditional_estimates(df):\n",
    "    conditional = []\n",
    "    for f,l in df[['first_name','last_name']].values.astype(str):\n",
    "        try:\n",
    "            response = genni_ethnea[f+'+'+l]\n",
    "            c = ethnea_codes[response['Ethnea'].split('-')[0]]\n",
    "            if c == 'XXX':\n",
    "                est = '-'\n",
    "            else:\n",
    "                est = convert_ratio_to_class(aggregates[f]['c'][c][0])\n",
    "        except:\n",
    "            est = '-'\n",
    "        conditional.append(est)\n",
    "    return conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df,_ in benchmarks:\n",
    "    df['genni_b'] = compute_genni_estimates(df)\n",
    "    df['country_b'] = compute_conditional_estimates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_performance(preds, n = 10000):\n",
    "    samples = []\n",
    "    for i in range(n):\n",
    "        d = preds.sample(frac=1,replace=True)\n",
    "        diffs = []\n",
    "        for m in ['country_b','genni_b']:\n",
    "            diffs.append(sum([x if x != '-' else y for x,y in d[[m,'cct_b']].values] == d['gender']) \n",
    "                       - sum(d['cct_b'] == d['gender']))\n",
    "        samples.append(diffs)\n",
    "    samples = np.array(samples).T\n",
    "    d = preds\n",
    "    diffs = []\n",
    "    for m in ['country_b','genni_b']:\n",
    "        diffs.append(sum([x if x != '-' else y for x,y in d[[m,'cct_b']].values] == d['gender']) \n",
    "                   - sum(d['cct_b'] == d['gender']))\n",
    "    return [(diffs[0], np.quantile(samples[0], q = [.025,.975])),\n",
    "            (diffs[1], np.quantile(samples[1], q = [.025,.975])),\n",
    "            len(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditional_performance = []\n",
    "for df,_ in benchmarks:\n",
    "    conditional_performance.append(compute_conditional_performance(df.loc[df['l'] == 'Conditionally Gendered (country)',\n",
    "                                           ['cct_b','genni_b','country_b','gender']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.rcParams['font.size'] = 16\n",
    "colors = [combined_colors[x] for x in ['purple','blue','green','orange','yellow','red']]\n",
    "for i,performance in enumerate(conditional_performance):\n",
    "    if i == 0:\n",
    "        plt.scatter(i-.1,performance[0][0],color='.1',label='Conditioned',s=50,marker='o')\n",
    "        plt.scatter(i+.1,performance[1][0],color='.1',label='Ethnea',s=50,marker='v')\n",
    "    else:\n",
    "        plt.scatter(i-.1,performance[0][0],color='.1',s=50,marker='o')\n",
    "        plt.scatter(i+.1,performance[1][0],color='.1',s=50,marker='v')\n",
    "        \n",
    "    plt.plot([i-.1,i-.1],performance[0][1],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i-.1-.05,i-.1+.05],\n",
    "             [performance[0][1][0],performance[0][1][0]],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i-.1-.05,i-.1+.05],\n",
    "             [performance[0][1][1],performance[0][1][1]],color='.1',zorder=2.5,lw=1)\n",
    "\n",
    "    plt.plot([i+.1,i+.1],performance[1][1],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i+.1-.05,i+.1+.05],\n",
    "             [performance[1][1][0],performance[1][1][0]],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i+.1-.05,i+.1+.05],\n",
    "             [performance[1][1][1],performance[1][1][1]],color='.1',zorder=2.5,lw=1)\n",
    "    \n",
    "plt.axhline(0,c=np.ones(3)*44/256,alpha=.8,lw=3,zorder=0,ls='--')\n",
    "\n",
    "plt.ylabel('# of Additional\\nCorrect Classifications')\n",
    "\n",
    "plt.xticks([0,1,2], ['Santamaría\\n('r'$N=%d$)'%conditional_performance[0][-1],\n",
    "                     'Vogel\\n('r'$N=%d$)'%conditional_performance[1][-1],\n",
    "                     'Morgan\\n('r'$N=%d$)'%conditional_performance[2][-1]])\n",
    "plt.xlim(-.2,2.2)\n",
    "plt.ylim(-34,18)\n",
    "\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.legend(loc='lower right',handletextpad=.4,handlelength=.8)\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "# ax.text(-0.14, 1.03, 'B', transform=ax.transAxes, fontname='Helvetica',\n",
    "#                     fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "\n",
    "ax.text(.292, .66, 'CCT', transform=ax.transAxes, fontname='Helvetica',\n",
    "                    fontsize=16, fontweight='regular', va='bottom', ha='center',c=np.ones(3)*44/256)\n",
    "\n",
    "\n",
    "plt.plot([.14,.35],[conditional_performance[0][1][0] +.98, conditional_performance[0][1][0] + 6],c='k',lw=1)\n",
    "plt.annotate(''r'$+%.1f$'%(100*conditional_performance[0][1][0]/conditional_performance[0][2])+\"%\",\n",
    "             xy=(.35+.025,conditional_performance[0][1][0] + 6),va=\"bottom\",ha=\"left\",fontsize=12)\n",
    "\n",
    "plt.plot([1-.04-.1,1-.25-.1],[conditional_performance[1][0][0] -.98, conditional_performance[1][0][0] - 6],c='k',lw=1)\n",
    "plt.annotate(''r'$%.1f$'%(100*conditional_performance[1][0][0]/conditional_performance[1][2])+\"%\",\n",
    "             xy=(1-.25-.1,conditional_performance[1][0][0] - 6),va=\"top\",ha=\"right\",fontsize=12)\n",
    "\n",
    "plt.plot([2-.14,1.65],[conditional_performance[2][0][0] +.98, conditional_performance[2][0][0] + 6],c='k',lw=1)\n",
    "plt.annotate(''r'$+%.1f$'%(100*conditional_performance[2][0][0]/conditional_performance[2][2])+\"%\",\n",
    "             xy=(1.65,conditional_performance[2][0][0] + 6),va=\"bottom\",ha=\"right\",fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig('../figures/delta_matching_using_country.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of conditionally gendered that are country dependent\n",
    "for df,_ in benchmarks:\n",
    "    conditioned = df.loc[(df['l'].str.contains('Conditionally Gendered'))]\n",
    "    print(df.loc[(df['l'] == 'Conditionally Gendered (country)')].shape[0]/conditioned.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often do classifications change when we condition on country\n",
    "same = [0,0]\n",
    "tot = [0,0]\n",
    "for df,_ in benchmarks:\n",
    "    conditioned = df.loc[(df['l'] == 'Conditionally Gendered (country)')]\n",
    "    c,g = conditioned.loc[(conditioned['genni_b'].isin(['m','f']))&\n",
    "                (conditioned['cct_b'].isin(['m','f'])),['cct_b','genni_b']].values.T\n",
    "    same[0] += np.sum(c == g)\n",
    "    tot[0] += len(c)\n",
    "    \n",
    "    c,g = conditioned.loc[(conditioned['country_b'].isin(['m','f']))&\n",
    "                (conditioned['cct_b'].isin(['m','f'])),['cct_b','country_b']].values.T\n",
    "    same[1] += np.sum(c == g)\n",
    "    tot[1] += len(c)\n",
    "    \n",
    "1-np.array(same)/np.array(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how often do jean and andrea change\n",
    "all_df = pd.concat([df.loc[(df['l'] == 'Conditionally Gendered (country)'),\n",
    "                           ['first_name','gender','l','cct_b','genni_b','country_b']] for df,_ in benchmarks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df.loc[(all_df['cct_b'].isin(['m','f']))&\n",
    "           (all_df['country_b'].isin(['m','f']))&\n",
    "           (all_df['cct_b']!=all_df['country_b']),'first_name'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.loc[(all_df['cct_b'].isin(['m','f']))&\n",
    "           (all_df['country_b'].isin(['m','f'])),'first_name'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(15+9)/(83+39) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_missing_performance(preds, n = 2000):\n",
    "    missing = preds.loc[preds['l']=='No Data',['namsor_b', 'gender']]\n",
    "    samples = []\n",
    "    for i in range(n):\n",
    "        d = missing.sample(frac=1,replace=True)\n",
    "        samples.append((np.sum(d['namsor_b'] == d['gender']) - np.sum(d['gender'] == 'm')))\n",
    "        \n",
    "    d = missing\n",
    "    diff = (np.sum(d['namsor_b'] == d['gender']) - np.sum(d['gender'] == 'm'))\n",
    "    return [(diff, np.quantile(samples, q = [.025,.975])),len(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vogel['namsor_b'] = [convert_ratio_to_class(r) for r in class_comps['namsor']['vogel']]\n",
    "morgan['namsor_b'] = [convert_ratio_to_class(r) for r in class_comps['namsor']['morgan']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_performance = []\n",
    "for df,_ in benchmarks:    \n",
    "    missing_performance.append(compute_missing_performance(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.rcParams['font.size'] = 16\n",
    "colors = [combined_colors[x] for x in ['purple','blue','green','orange','yellow','red']]#['b','r','g','k','y']\n",
    "\n",
    "for i,performance in enumerate(missing_performance):\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.scatter(i,performance[0][0],color='.1',label='Namsor',s=50,marker='o')\n",
    "    else:\n",
    "        plt.scatter(i,performance[0][0],color='.1',s=50,marker='o')\n",
    "    plt.plot([i,i],performance[0][1],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i-.05,i+.05],\n",
    "             [performance[0][1][0],performance[0][1][0]],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i-.05,i+.05],\n",
    "             [performance[0][1][1],performance[0][1][1]],color='.1',zorder=2.5,lw=1)\n",
    "\n",
    "\n",
    "\n",
    "plt.axhline(0,c=colors[-1],alpha=.8,lw=3,zorder=0,ls='--')\n",
    "\n",
    "plt.ylabel('# of Additional\\nCorrect Classifications')\n",
    "# plt.yticks([],[])\n",
    "\n",
    "plt.xticks([0,1,2], ['Santamaría\\n('r'$N=%d$)'%missing_performance[0][-1],\n",
    "                     'Vogel\\n('r'$N=%d$)'%missing_performance[1][-1],\n",
    "                     'Morgan\\n('r'$N=%d$)'%missing_performance[2][-1]])\n",
    "\n",
    "\n",
    "plt.xlim(-.1,2.1)\n",
    "plt.ylim(-22,23)\n",
    "\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.legend(loc='lower right',handletextpad=.4,handlelength=.8)\n",
    "plt.rcParams['font.size'] = 16\n",
    "\n",
    "# ax.text(-0.14, 1.03, 'C', transform=ax.transAxes, fontname='Helvetica',\n",
    "#                     fontsize=18, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "ax.text(.265, .5, 'Guess\\nGendered Male', transform=ax.transAxes, fontname='Helvetica',\n",
    "                    fontsize=16, fontweight='regular', va='bottom', ha='center',c=colors[-1])\n",
    "\n",
    "\n",
    "plt.plot([.04,.25],[missing_performance[0][0][0] -.98, missing_performance[0][0][0] - 6],c='k',lw=1)\n",
    "plt.annotate(''r'$%.1f$'%(100*missing_performance[0][0][0]/missing_performance[0][1])+\"%\",\n",
    "             xy=(.25,missing_performance[0][0][0] - 6),va=\"top\",ha=\"left\",fontsize=12)\n",
    "\n",
    "plt.plot([1.04,1.25],[missing_performance[1][0][0] -.98, missing_performance[1][0][0] - 6],c='k',lw=1)\n",
    "plt.annotate(''r'$%.1f$'%(100*missing_performance[1][0][0]/missing_performance[1][1])+\"%\",\n",
    "             xy=(1.25,missing_performance[1][0][0] - 6),va=\"top\",ha=\"left\",fontsize=12)\n",
    "\n",
    "plt.plot([2-.04,1.75],[missing_performance[2][0][0] +.98, missing_performance[2][0][0] + 6],c='k',lw=1)\n",
    "plt.annotate(''r'$+%.1f$'%(100*missing_performance[2][0][0]/missing_performance[2][1])+\"%\",\n",
    "             xy=(1.75,missing_performance[2][0][0] + 6),va=\"bottom\",ha=\"right\",fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig('../figures/delta_matching_using_namsor.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Namsor correspondence on no data names\n",
    "for df,_ in benchmarks:\n",
    "    n,g = df.loc[df['l']=='No Data',['namsor_b', 'gender']].values.T\n",
    "    print(np.round(100*np.mean(n==g),1), np.round(100*np.mean('m'==g),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namsor confidence\n",
    "all_labels = np.array(list(itertools.chain.from_iterable([df['l'].values for df,_ in benchmarks])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_namsor = np.array(list(itertools.chain.from_iterable([v for k,v in class_comps['namsor'].items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2*abs(.5 - all_namsor))[all_labels == 'Weakly Gendered'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2*abs(.5 - all_namsor))[all_labels != 'Weakly Gendered'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_performance(preds, n = 2000):\n",
    "    avg = preds[['avg_b','cct_b', 'gender']]\n",
    "    samples = []\n",
    "    for i in range(n):\n",
    "        d = avg.sample(frac=1,replace=True)\n",
    "        samples.append((np.sum(d['avg_b'] == d['gender']) - np.sum(d['cct_b'] == d['gender'])))\n",
    "        \n",
    "    d = avg\n",
    "\n",
    "    diff = (np.sum(d['avg_b'] == d['gender']) - np.sum(d['cct_b'] == d['gender']))\n",
    "    return [(diff, np.quantile(samples, q = [.025,.975])),len(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_performance = []\n",
    "for df,_ in benchmarks:    \n",
    "    avg_performance.append(compute_avg_performance(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.rcParams['font.size'] = 16\n",
    "colors = [combined_colors[x] for x in ['purple','blue','green','orange','yellow','red']]#['b','r','g','k','y']\n",
    "\n",
    "    \n",
    "for i,performance in enumerate(avg_performance):\n",
    "    \n",
    "    if i == 0:\n",
    "        plt.scatter(i,performance[0][0],color='.1',label='Average',s=50,marker='o')\n",
    "    else:\n",
    "        plt.scatter(i,performance[0][0],color='.1',s=50,marker='o')\n",
    "    plt.plot([i,i],performance[0][1],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i-.05,i+.05],\n",
    "             [performance[0][1][0],performance[0][1][0]],color='.1',zorder=2.5,lw=1)\n",
    "    plt.plot([i-.05,i+.05],\n",
    "             [performance[0][1][1],performance[0][1][1]],color='.1',zorder=2.5,lw=1)\n",
    "    \n",
    "    \n",
    "plt.axhline(0,c=colors[0],alpha=.8,lw=3,zorder=0,ls='--')\n",
    "plt.ylabel(''r'$\\Delta$ Matching Classifications')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.xticks([0,1,2], ['Santamaría\\n('r'$N=%d$)'%avg_performance[0][-1],\n",
    "                     'Vogel\\n('r'$N=%d$)'%avg_performance[1][-1],\n",
    "                     'Morgan\\n('r'$N=%d$)'%avg_performance[2][-1]])\n",
    "plt.xlim(-.2,2.2)\n",
    "\n",
    "plt.ylabel('# of Additional\\nCorrect Classifications')\n",
    "\n",
    "ax.text(.292, .665, 'CCT', transform=ax.transAxes, fontname='Helvetica',\n",
    "                    fontsize=16, fontweight='regular', va='bottom', ha='center',c=colors[0])\n",
    "\n",
    "\n",
    "plt.plot([.04,.25],[avg_performance[0][0][0] -.98, avg_performance[0][0][0] - 6],c='k',lw=1)\n",
    "plt.annotate(''r'$%.1f$'%(100*avg_performance[0][0][0]/avg_performance[0][1])+\"%\",\n",
    "             xy=(.25,avg_performance[0][0][0] - 6),va=\"top\",ha=\"left\",fontsize=12)\n",
    "\n",
    "plt.plot([1.04,1.25],[avg_performance[1][0][0] +.98, avg_performance[1][0][0] + 6],c='k',lw=1)\n",
    "plt.annotate(''r'$%.1f$'%(100*avg_performance[1][0][0]/avg_performance[1][1])+\"%\",\n",
    "             xy=(1.25,avg_performance[1][0][0] + 6),va=\"bottom\",ha=\"left\",fontsize=12)\n",
    "\n",
    "plt.plot([2-.04,1.75],[avg_performance[2][0][0] +.98, avg_performance[2][0][0] + 6],c='k',lw=1)\n",
    "plt.annotate(''r'$+%.1f$'%(100*avg_performance[2][0][0]/avg_performance[2][1])+\"%\",\n",
    "             xy=(1.75,avg_performance[2][0][0] + 6),va=\"bottom\",ha=\"right\",fontsize=12)\n",
    "\n",
    "\n",
    "plt.savefig('../figures/delta_matching_cct_avg.png',facecolor='white',bbox_inches='tight',dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssa coverage and % classified\n",
    "def ssa_average(n):\n",
    "    try:\n",
    "        ssa = source_associations[n]['1']\n",
    "        return ssa['f']/(ssa['f']+ssa['m'])\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "santa['ssa_b'] = [convert_ratio_to_class(ssa_average(parse_name(n))) for n in santa['first_name']]\n",
    "\n",
    "x,y = santa.loc[santa['ssa_b'].isin(['m','f']),['gender','ssa_b']].values.T\n",
    "np.mean(x == y), len(x)/len(santa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg coverage and % classified\n",
    "x,y = santa.loc[santa['avg_b'].isin(['m','f']),['gender','avg_b']].values.T\n",
    "np.mean(x == y), len(x)/len(santa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg coverage and % classified (only gendered)\n",
    "x,y = santa.loc[(santa['avg_b'].isin(['m','f']))&\n",
    "                (santa['l'].isin(['Gendered (high coverage)','Gendered (low coverage)'])),\n",
    "                ['gender','avg_b']].values.T\n",
    "np.mean(x == y), len(x)/len(santa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg matches cct\n",
    "np.mean(santa['avg_b'] == santa['cct_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many gendered male and female fall into gendered (high coverage) subset\n",
    "for df, _ in benchmarks:\n",
    "    kept = []\n",
    "    for g in ['f', 'm']:\n",
    "        kept.append(df.loc[(df['gender']==g)&\n",
    "                  (df['l']=='Gendered (high coverage)')].shape[0]/sum(df['gender'] == g))\n",
    "    print(kept,kept[0]-kept[1])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# composition estimates\n",
    "for df, _ in benchmarks:\n",
    "    df_g = df.loc[(df['l']=='Gendered (high coverage)')]\n",
    "    print(100*np.round((df['gender'] == 'm').mean(),5) ,\n",
    "          100*np.round((df_g.loc[df_g['avg_b'].isin(['m','f']),'avg_b'] == 'm').mean(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error rates by group\n",
    "def compute_gender_errors(preds):\n",
    "    f = sum(preds['gender'] == 'f')\n",
    "    m = sum(preds['gender'] == 'm')\n",
    "    fm = sum((preds['gender'] == 'f')&(preds['avg_b'] == 'm'))\n",
    "    mf = sum((preds['gender'] == 'm')&(preds['avg_b'] == 'f'))\n",
    "    return(np.round(100*fm/f,5) - np.round(100*mf/m,5))\n",
    "\n",
    "print(compute_gender_errors(santa))\n",
    "\n",
    "print(compute_gender_errors(vogel))\n",
    "\n",
    "print(compute_gender_errors(morgan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall error rates\n",
    "all_benchmarks = pd.concat([x[0] for x in benchmarks])\n",
    "for g,e in zip(['m', 'f'],['f', 'm']):\n",
    "    print(100*np.mean(all_benchmarks.loc[(all_benchmarks['gender'] == g),'avg_b'] == e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nationality issues\n",
    "c_l_pairs = []\n",
    "for f,l,label in santa[['first_name','last_name','l']].values:\n",
    "    try:\n",
    "        response = genni_ethnea[f+'+'+l]\n",
    "        c = response['Ethnea'].split('-')[0]\n",
    "        c_l_pairs.append((c,label))\n",
    "    except:\n",
    "        contiue\n",
    "        \n",
    "def country_info(c):\n",
    "    d = [(x,y) for x,y in c_l_pairs if x == c]\n",
    "    n = len([y for x,y in d if (y == 'Weakly Gendered' or y == 'No Data')])\n",
    "    return n/len(d)\n",
    "\n",
    "country_info('CHINESE'), country_info('ENGLISH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra bit on male / female names\n",
    "for g in ['m','f']:\n",
    "    gtx = 100 * pd.concat((pd.DataFrame(santa.loc[santa['gender']==g].groupby('l').count()['first_name']\n",
    "                                        /len(santa.loc[santa['gender']==g])),\n",
    "    pd.DataFrame(vogel.loc[vogel['gender']==g].groupby('l').count()['first_name']\n",
    "                 /len(vogel.loc[vogel['gender']==g])),\n",
    "    pd.DataFrame(morgan.loc[morgan['gender']==g].groupby('l').count()['first_name']\n",
    "                 /len(morgan.loc[morgan['gender']==g]))), axis = 1)\n",
    "    gtx.columns = ['santa','vogel','morgan']\n",
    "    print(gtx.round(1).loc[tx_labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anonymized_gender_rep",
   "language": "python",
   "name": "anonymized_gender_rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
